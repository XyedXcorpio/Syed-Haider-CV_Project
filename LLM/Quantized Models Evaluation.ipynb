{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4s8WgOcHgH-"
      },
      "source": [
        "# **Problem Statement**\n",
        "\n",
        "*In todayâ€™s fast-paced digital world, companies need to provide accurate and fast answers to frequently asked questions (FAQs) while keeping the computational costs low. In this lab, you will develop an efficient FAQ Question Answering (QA) system by fine-tuning a lightweight student model using knowledge distillation from a high-performing teacher model. You will then apply dynamic quantization to the student model to reduce its memory footprint and inference latency without degrading its performance.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeCvFmXRNuQI"
      },
      "source": [
        "## Challenge:\n",
        "Experiment with different training epochs until the fine-tuned student model can reliably answer the test question, \"On Saturdays, when do you open your office?\" while maintaining efficiency gains after quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D3OEcC3OAhJ"
      },
      "source": [
        "Import Libraries and Set Up Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeqVDs2vOH4_",
        "outputId": "3c484fb5-ad52-4a1a-ea32-bc3a5415aaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas datasets torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12GR0QDhHfT8",
        "outputId": "e2cbc361-d911-4b4f-921a-4af85c253c31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ],
      "source": [
        "# This cell imports all necessary libraries and sets up the environment.\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "import time\n",
        "import torch.quantization\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-O-WpUIOh3u"
      },
      "source": [
        "## Prepare the Dataset\n",
        "The dataset contains 51 distinct questions and answers for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8nG1p_2OhRt",
        "outputId": "a75013d0-4e8b-4cce-baa3-1a628cccf621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique QA pairs: 51\n",
            "Dataset size: 51 examples\n"
          ]
        }
      ],
      "source": [
        "# This cell creates a dataset of 51 unique FAQ pairs.\n",
        "# The dataset contains 51 distinct questions and answers for fine-tuning.\n",
        "questions = [\n",
        "    \"What are your opening hours?\",\n",
        "    \"How can I reset your password?\",\n",
        "    \"Where is your store located?\",\n",
        "    \"What is the return policy?\",\n",
        "    \"How do I track my order?\",\n",
        "    \"Do you offer free shipping?\",\n",
        "    \"How do I create an account?\",\n",
        "    \"What payment methods do you accept?\",\n",
        "    \"Can I cancel my order?\",\n",
        "    \"How do I exchange a product?\",\n",
        "    \"What warranty do your products have?\",\n",
        "    \"How do I subscribe to your newsletter?\",\n",
        "    \"What are the shipping costs?\",\n",
        "    \"Where can I find the latest deals?\",\n",
        "    \"Do you have a loyalty program?\",\n",
        "    \"How do I use a promo code?\",\n",
        "    \"Can I place an order over the phone?\",\n",
        "    \"What is your privacy policy?\",\n",
        "    \"How do I update my billing information?\",\n",
        "    \"What is your refund policy?\",\n",
        "    \"How do I contact customer support?\",\n",
        "    \"Do you have gift cards available?\",\n",
        "    \"What are your international shipping options?\",\n",
        "    \"How can I apply for a job at your company?\",\n",
        "    \"Do you offer same-day delivery?\",\n",
        "    \"How do I return a defective product?\",\n",
        "    \"What are your holiday hours?\",\n",
        "    \"Where can I view product reviews?\",\n",
        "    \"Can I pre-order new products?\",\n",
        "    \"How do I report a problem with my order?\",\n",
        "    \"What measures do you take for product safety?\",\n",
        "    \"Do you offer installation services?\",\n",
        "    \"How do I find a store near me?\",\n",
        "    \"Can I get a price match guarantee?\",\n",
        "    \"What is your exchange policy?\",\n",
        "    \"Do you have financing options available?\",\n",
        "    \"How do I check my order status?\",\n",
        "    \"What is the process for bulk orders?\",\n",
        "    \"How do I reset my account password?\",\n",
        "    \"What is your policy on damaged goods?\",\n",
        "    \"How do I update my shipping address?\",\n",
        "    \"Are there any membership benefits?\",\n",
        "    \"What discounts do you offer for students?\",\n",
        "    \"How do I become a reseller?\",\n",
        "    \"Do you offer warranties on electronics?\",\n",
        "    \"What is the best way to contact your sales team?\",\n",
        "    \"Can I request a catalog?\",\n",
        "    \"How do I provide feedback on your service?\",\n",
        "    \"What is your procedure for handling complaints?\",\n",
        "    \"Where can I find instructions for product setup?\",\n",
        "    \"Do you offer virtual shopping assistance?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    \"Our store is open from 9 AM to 9 PM every day.\",\n",
        "    \"To reset your password, click 'Forgot Password' on the login page and follow the emailed instructions.\",\n",
        "    \"Our store is located at 123 Main Street in Anytown.\",\n",
        "    \"Products can be returned within 30 days with a valid receipt.\",\n",
        "    \"Log into your account and click 'Order History' to track your order.\",\n",
        "    \"Yes, free shipping is available on orders over $50.\",\n",
        "    \"Click 'Sign Up' on our homepage and fill in the registration form.\",\n",
        "    \"We accept major credit cards, PayPal, and Apple Pay.\",\n",
        "    \"Orders can be cancelled within one hour of placement.\",\n",
        "    \"To exchange a product, visit any of our store locations with your receipt.\",\n",
        "    \"Our products come with a one-year warranty for manufacturing defects.\",\n",
        "    \"Subscribe by entering your email address on our newsletter signup page.\",\n",
        "    \"Shipping costs vary by location and order size; please refer to our shipping policy.\",\n",
        "    \"Visit our 'Promotions' page to view the latest deals.\",\n",
        "    \"Yes, our loyalty program offers exclusive discounts and rewards.\",\n",
        "    \"Enter your promo code at checkout to receive your discount.\",\n",
        "    \"Yes, orders can be placed over the phone via our customer service hotline.\",\n",
        "    \"Our privacy policy is detailed on our website; we take data security seriously.\",\n",
        "    \"Update your billing information in your account settings under 'Billing'.\",\n",
        "    \"Refunds are processed within 7-10 business days after return.\",\n",
        "    \"You can contact customer support via our hotline or email support@ourstore.com.\",\n",
        "    \"Gift cards are available in multiple denominations.\",\n",
        "    \"We ship internationally to selected countries; see our international shipping page for details.\",\n",
        "    \"Visit our careers page for job openings and application procedures.\",\n",
        "    \"Same-day delivery is available in select areas.\",\n",
        "    \"If a product is defective, contact support immediately for a replacement or refund.\",\n",
        "    \"Holiday hours are posted on our website during seasonal periods.\",\n",
        "    \"Product reviews can be found on the product detail pages of our website.\",\n",
        "    \"Pre-orders for upcoming products are available online.\",\n",
        "    \"Report any order issues by contacting our customer service department.\",\n",
        "    \"We follow strict quality control standards to ensure product safety.\",\n",
        "    \"Yes, installation services are offered for select products.\",\n",
        "    \"Use our store locator on the website to find the nearest store.\",\n",
        "    \"We offer a price match guarantee if you find a lower price elsewhere.\",\n",
        "    \"Exchanges are allowed within 30 days under our exchange policy.\",\n",
        "    \"Financing options are available through our partnered financial services.\",\n",
        "    \"Check your order status by logging into your account and selecting 'Order Status'.\",\n",
        "    \"For bulk orders, please contact our sales team for a custom quote.\",\n",
        "    \"If you've forgotten your account password, use the 'Forgot Password' link on the login page.\",\n",
        "    \"Our damaged goods policy allows returns or exchanges within 30 days with proof of damage.\",\n",
        "    \"Update your shipping address in your account settings or by contacting support.\",\n",
        "    \"Membership benefits include exclusive discounts and early access to new products.\",\n",
        "    \"We offer a 10% discount for students with a valid student ID.\",\n",
        "    \"To become a reseller, complete the reseller application form on our website.\",\n",
        "    \"Yes, extended warranties on electronics are available for purchase.\",\n",
        "    \"Contact our sales team via the 'Contact Us' page for prompt assistance.\",\n",
        "    \"You can request a catalog by filling out our online catalog request form.\",\n",
        "    \"Provide feedback through our online feedback form or by calling customer service.\",\n",
        "    \"We have a dedicated process to handle complaints efficiently; please contact support.\",\n",
        "    \"Setup instructions are included with your product and available online.\",\n",
        "    \"Virtual shopping assistance is available via our online chat service.\"\n",
        "]\n",
        "\n",
        "print(f\"Total unique QA pairs: {len(questions)}\")\n",
        "data = {\"question\": questions, \"answer\": answers}\n",
        "faq_df = pd.DataFrame(data)\n",
        "faq_dataset = Dataset.from_pandas(faq_df)\n",
        "print(f\"Dataset size: {len(faq_dataset)} examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEkKUv7FO_fE"
      },
      "source": [
        "## Load Teacher and Student Models\n",
        "The teacher model is pre-trained on SQuAD and is expected to produce accurate QA outputs.\n",
        "You will learn to mimic the teacher while being more efficient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnWc0qDjHfWf",
        "outputId": "ac4889fa-ed1e-42f4-8505-1b9c7b07c42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher and student models loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# This cell loads the teacher model and the student model.\n",
        "\n",
        "teacher_model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
        "teacher_model = AutoModelForQuestionAnswering.from_pretrained(teacher_model_name)\n",
        "\n",
        "# The student model is a lightweight variant that will be fine-tuned.\n",
        "student_model_name = \"distilbert-base-uncased\"\n",
        "student_model = AutoModelForQuestionAnswering.from_pretrained(student_model_name)\n",
        "\n",
        "print(\"Teacher and student models loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9k9JM8_P5cb"
      },
      "source": [
        "## Define Helper Functions\n",
        " 1. distillation_loss: Combines the teacher's soft predictions and hard labels.\n",
        " 2. compute_answer_span: Extracts the true answer span from tokenized input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnOgkReoHfZF",
        "outputId": "6888baf5-5c68-47e7-a5ea-cd6eef473c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def distillation_loss(student_start, student_end, teacher_start, teacher_end, true_start, true_end, alpha=0.5, temperature=2.0):\n",
        "    # Soft target loss using KL divergence.\n",
        "    soft_loss_start = F.kl_div(\n",
        "        F.log_softmax(student_start / temperature, dim=-1),\n",
        "        F.softmax(teacher_start / temperature, dim=-1),\n",
        "        reduction=\"batchmean\"\n",
        "    ) * (temperature ** 2)\n",
        "    soft_loss_end = F.kl_div(\n",
        "        F.log_softmax(student_end / temperature, dim=-1),\n",
        "        F.softmax(teacher_end / temperature, dim=-1),\n",
        "        reduction=\"batchmean\"\n",
        "    ) * (temperature ** 2)\n",
        "    # Hard target loss using cross entropy.\n",
        "    hard_loss_start = F.cross_entropy(student_start, true_start)\n",
        "    hard_loss_end = F.cross_entropy(student_end, true_end)\n",
        "    # Combine the losses.\n",
        "    return alpha * ((soft_loss_start + soft_loss_end) / 2) + (1 - alpha) * ((hard_loss_start + hard_loss_end) / 2)\n",
        "\n",
        "def compute_answer_span(single_input, tokenizer):\n",
        "    # This function assumes the SQuAD-style input: [CLS] question tokens [SEP] context tokens [SEP] ...\n",
        "    input_ids = single_input['input_ids'][0].tolist()\n",
        "    sep_id = tokenizer.sep_token_id\n",
        "    try:\n",
        "        sep_index = input_ids.index(sep_id)\n",
        "        second_sep_index = input_ids.index(sep_id, sep_index + 1)\n",
        "    except ValueError:\n",
        "        sep_index = 0\n",
        "        second_sep_index = len(input_ids) - 1\n",
        "    true_start = sep_index + 1  # Start of context tokens.\n",
        "    true_end = second_sep_index - 1  # End of context tokens.\n",
        "    return true_start, true_end\n",
        "\n",
        "print(\"Helper functions defined.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CkRze_BQVHv"
      },
      "source": [
        "## Fine-Tuning via Knowledge Distillation\n",
        "\n",
        "It processes the dataset, tokenizes the inputs, computes the true answer span,\n",
        "and trains the student model to match the teacher's outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkAzvpJHfbM",
        "outputId": "df830c28-28c8-4fae-c82c-f501e7646a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed. Loss: 3.1602189540863037\n",
            "Epoch 2 completed. Loss: 1.5478594303131104\n",
            "Epoch 3 completed. Loss: 1.761122226715088\n",
            "Epoch 4 completed. Loss: 0.44907593727111816\n",
            "Epoch 5 completed. Loss: 1.2566320896148682\n",
            "Epoch 6 completed. Loss: 0.5907028913497925\n",
            "Epoch 7 completed. Loss: 0.6963661909103394\n",
            "Epoch 8 completed. Loss: 0.6079907417297363\n",
            "Epoch 9 completed. Loss: 0.5653973817825317\n",
            "Epoch 10 completed. Loss: 0.8307878971099854\n",
            "Epoch 11 completed. Loss: 0.4184042811393738\n",
            "Epoch 12 completed. Loss: 0.8936832547187805\n",
            "Epoch 13 completed. Loss: 0.9776091575622559\n",
            "Epoch 14 completed. Loss: 0.679531455039978\n",
            "Epoch 15 completed. Loss: 0.8041097521781921\n",
            "Epoch 16 completed. Loss: 0.3470720052719116\n",
            "Epoch 17 completed. Loss: 0.7869094610214233\n",
            "Epoch 18 completed. Loss: 0.8783726692199707\n",
            "Epoch 19 completed. Loss: 0.7106615304946899\n",
            "Epoch 20 completed. Loss: 0.8415277600288391\n",
            "Fine-tuning complete. Adjust Epochs if needed to improve performance.\n"
          ]
        }
      ],
      "source": [
        "# This cell fine-tunes the student model using knowledge distillation.\n",
        "\n",
        "train_loader = DataLoader(faq_dataset, batch_size=2, shuffle=True)\n",
        "student_model.train()\n",
        "optimizer = torch.optim.Adam(student_model.parameters(), lr=3e-5)\n",
        "\n",
        "# Experiment with different numbers of epochs.\n",
        "Epochs = 20  # Try different values (e.g., 10, 30, 50) until the model answers reliably.\n",
        "\n",
        "for epoch in range(Epochs):\n",
        "    for batch in train_loader:\n",
        "        # Tokenize both the question and the answer (the answer acts as context).\n",
        "        batch_inputs = tokenizer(batch[\"question\"], batch[\"answer\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
        "        true_starts, true_ends = [], []\n",
        "        # Compute the correct answer span for each sample.\n",
        "        for i in range(len(batch[\"question\"])):\n",
        "            single_input = {k: v[i].unsqueeze(0) for k, v in batch_inputs.items()}\n",
        "            ts, te = compute_answer_span(single_input, tokenizer)\n",
        "            true_starts.append(ts)\n",
        "            true_ends.append(te)\n",
        "        true_start = torch.tensor(true_starts)\n",
        "        true_end = torch.tensor(true_ends)\n",
        "\n",
        "        # Obtain teacher outputs without computing gradients.\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(**batch_inputs)\n",
        "            teacher_start = teacher_outputs.start_logits\n",
        "            teacher_end = teacher_outputs.end_logits\n",
        "        # Forward pass through the student model.\n",
        "        student_outputs = student_model(**batch_inputs)\n",
        "        student_start = student_outputs.start_logits\n",
        "        student_end = student_outputs.end_logits\n",
        "\n",
        "        # Compute the distillation loss.\n",
        "        loss = distillation_loss(student_start, student_end, teacher_start, teacher_end, true_start, true_end)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
        "\n",
        "print(\"Fine-tuning complete. Adjust Epochs if needed to improve performance.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNOe-i5PeeOr"
      },
      "source": [
        "## *Important*:\n",
        "### *Dynamic quantization in PyTorch is only supported on the CPU. This means that after applying dynamic quantization to your fine-tuned student model, the quantized model's operators (such as quantized::linear_dynamic) are implemented only for CPU. If you try to run inference with a quantized model on a GPU, you'll encounter errors*.\n",
        "\n",
        "### *Therefore*:\n",
        "\n",
        "### *All models must be moved to the CPU for a fair comparison*.\n",
        "### *Use .cpu() to move your models to the CPU*.\n",
        "### *When creating the QA pipeline, pass device=-1 to force CPU usage.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g-7-qZrXkX1"
      },
      "source": [
        "##Apply Dynamic Quantization[TODO]\n",
        "\n",
        " In this cell, you will apply dynamic quantization to the  fine-tuned student model. Quantization converts parts of the\n",
        " model (such as Linear layers) to lower precision (e.g., int8),\n",
        " which reduces model size and speeds up inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ux-lPggtXaXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cccf66-cac1-4db8-d742-a5b670cfb4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic quantization applied to the student model.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.quantization\n",
        "# Apply dynamic quantization to the student_model for the torch.nn.Linear layers.\n",
        "\n",
        "# TODO: Apply dynamic quantization to the student_model for the torch.nn.Linear layers.\n",
        "\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    student_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8)\n",
        "\n",
        "# TODO: Set teacher_model, student_model and quantized_model to evaluation mode.\n",
        "\n",
        "teacher_model.eval()\n",
        "student_model.eval()\n",
        "quantized_model.eval()\n",
        "print(\"Dynamic quantization applied to the student model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tiJohecX5gl"
      },
      "source": [
        "## Inference and Evaluation[TODO]\n",
        "\n",
        "This cell sets up a question-answering (QA) pipeline and\n",
        "evaluates the teacher model, fine-tuned student model, and\n",
        "quantized student model on a given context and question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mviP4ATHX381",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b909d66-56ed-4425-c117-055fdbcbcd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Model Answer:\n",
            "{'score': 0.5255181789398193, 'start': 172, 'end': 185, 'answer': '10 AM to 6 PM'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fine-tuned Student Model Answer:\n",
            "{'score': 0.024858662858605385, 'start': 172, 'end': 199, 'answer': '10 AM to 6 PM on Saturdays.'}\n",
            "\n",
            "Quantized Student Model Answer:\n",
            "{'score': 0.017275527119636536, 'start': 172, 'end': 199, 'answer': '10 AM to 6 PM on Saturdays.'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define the inference context and question.\n",
        "context = (\n",
        "    \"Our office is located in the heart of downtown with modern facilities and a friendly environment. \"\n",
        "    \"We operate from 9 AM to 9 PM on weekdays (Monday through Friday) and from 10 AM to 6 PM on Saturdays. \"\n",
        "    \"Our team is dedicated to providing exceptional customer service. For further inquiries, please contact our customer support.\"\n",
        ")\n",
        "question = \"On Saturdays, when do you open your office?\"\n",
        "\n",
        "def answer_query(model, question, context, tokenizer):\n",
        "  qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device =-1)\n",
        "    # TODO: Create a QA pipeline for the given model using the 'pipeline' function.\n",
        "\n",
        "    # TODO: Return the answer by running the pipeline on the provided question and context.\n",
        "  return qa_pipe(question=question, context=context)\n",
        "\n",
        "# TODO: Evaluate the teacher model and store its answer.\n",
        "print(\"Teacher Model Answer:\")\n",
        "teacher_answer = answer_query(teacher_model, question, context, tokenizer)\n",
        "print(teacher_answer)\n",
        "\n",
        "# TODO: Evaluate the fine-tuned student model and store its answer.\n",
        "student_answer = answer_query(student_model, question, context, tokenizer)\n",
        "print(\"\\nFine-tuned Student Model Answer:\")\n",
        "print(student_answer)\n",
        "\n",
        "# TODO: Evaluate the quantized student model and store its answer.\n",
        "quantized_answer = answer_query(quantized_model, question, context, tokenizer)\n",
        "print(\"\\nQuantized Student Model Answer:\")\n",
        "print(quantized_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxgTiKA_h6OS"
      },
      "source": [
        "## Measure Inference Time and Model Size[TODO]\n",
        "\n",
        "In this cell, you will measure:\n",
        "   - The average inference time for each model over multiple runs.\n",
        "   - The approximate model size based on the sum of parameter sizes.\n",
        "\n",
        " These metrics help you compare the computational efficiency and\n",
        " memory footprint of the teacher, fine-tuned student, and quantized models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OG5Nu1dsXaaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d009537e-6aa1-4d8a-cde8-50469dd9a69e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Inference Time (seconds):\n",
            "Teacher Model: 0.0963\n",
            "Fine-tuned Student Model: 0.0931\n",
            "Quantized Student Model: 0.0569\n",
            "\n",
            "Approximate Model Size (MB):\n",
            "Teacher Model: 253.16 MB\n",
            "Fine-tuned Student Model: 253.16 MB\n",
            "Quantized Student Model: 91.00 MB\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def measure_inference_time(model, question, context, tokenizer, runs=10):\n",
        "  qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device =-1)\n",
        "\n",
        "    # TODO: Create a QA pipeline for the given model.\n",
        "\n",
        "    # Perform a few warm-up runs.\n",
        "  for _ in range(3):\n",
        "    _ = qa_pipe(question=question, context=context)\n",
        "    # TODO: Start a timer, run the pipeline 'runs' times, and stop the timer.\n",
        "  start_time = time.time()\n",
        "  for _ in range(runs):\n",
        "    _ = qa_pipe(question=question, context=context)\n",
        "  end_time = time.time()\n",
        "    # TODO: Calculate and return the average inference time.\n",
        "  avg_time = (end_time - start_time) / runs\n",
        "  return avg_time\n",
        "\n",
        "def get_model_size(model):\n",
        "  param_size = 0\n",
        "  for param in model.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "\n",
        "    # TODO: Iterate over all parameters in the model and sum their sizes (in bytes).\n",
        "\n",
        "    # Convert the total size from bytes to megabytes (MB) and return.\n",
        "  return param_size / (1024 ** 2)\n",
        "\n",
        "# TODO: Measure the average inference time for each model.\n",
        "teacher_time = measure_inference_time(teacher_model, question, context, tokenizer)\n",
        "student_time = measure_inference_time(student_model, question, context, tokenizer)\n",
        "quantized_time = measure_inference_time(quantized_model, question, context, tokenizer)\n",
        "\n",
        "\n",
        "# TODO: Calculate the approximate model sizes for each model.\n",
        "teacher_size = get_model_size(teacher_model)\n",
        "student_size = get_model_size(student_model)\n",
        "quantized_size = get_model_size(quantized_model)\n",
        "\n",
        "\n",
        "print(\"Average Inference Time (seconds):\")\n",
        "print(f\"Teacher Model: {teacher_time:.4f}\")\n",
        "print(f\"Fine-tuned Student Model: {student_time:.4f}\")\n",
        "print(f\"Quantized Student Model: {quantized_time:.4f}\")\n",
        "\n",
        "print(\"\\nApproximate Model Size (MB):\")\n",
        "print(f\"Teacher Model: {teacher_size:.2f} MB\")\n",
        "print(f\"Fine-tuned Student Model: {student_size:.2f} MB\")\n",
        "print(f\"Quantized Student Model: {quantized_size:.2f} MB\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}